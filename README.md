# Multi-Armed-Bandit-MAB

# HW3: Explore and Exploit for Arm-Bandit Problem

## ğŸ® ç’°å¢ƒä»‹ç´¹ï¼šå¤šè‡‚æ©Ÿå™¨äººï¼ˆMulti-Armed Banditï¼‰

æˆ‘å€‘å®šç¾©ä¸€å€‹ K è‡‚çš„å¤šè‡‚æ©Ÿå™¨äººç’°å¢ƒï¼Œå°ˆç‚ºæ¸¬è©¦æ¢ç´¢ç­–ç•¥åœ¨é›£ä»¥åˆ†è¾¨çš„ arm æƒ…å¢ƒä¸‹çš„è¡¨ç¾ã€‚ç’°å¢ƒè¨­è¨ˆè®“å¤§å¤šæ•¸ arm çš„æœŸæœ›å ±é…¬æ¥µç‚ºæ¥è¿‘ï¼Œåƒ…æœ‰ä¸€å€‹æœ€ä½³ arm ç¨å¾®å„ªæ–¼å…¶ä»–ï¼Œé©åˆä½œç‚ºè©•ä¼° Thompson Sampling ç­‰ç­–ç•¥çš„åŸºæº–ã€‚ç›®æ¨™æ˜¯é€éæ¼”ç®—æ³•åœ¨æœ‰é™æ¬¡è©¦é©—å…§è­˜åˆ¥æœ€ä½³ arm ä¸¦æœ€å¤§åŒ–ç¸½å ±é…¬ï¼ˆcumulative rewardï¼‰ã€‚
 - æ¯å€‹ arm çš„çå‹µæœå¾å¸¸æ…‹åˆ†å¸ƒ N(Î¼_i, 1)
 - ç¸½è©¦é©—æ¬¡æ•¸ï¼š10000 æ¬¡
 - arms æ•¸é‡ï¼š30 è‡‚
 - çœŸå¯¦æœŸæœ›å€¼ Î¼_i ç‚ºå¾ N(0, 0.01) éš¨æ©Ÿç”Ÿæˆ
 - éš¨æ©ŸæŒ‡å®šä¸€å€‹æœ€ä½³ armï¼Œå…¶æœŸæœ›å€¼é¡å¤–åŠ ä¸Š 0.25

![image](https://github.com/user-attachments/assets/ff0e27d3-48a6-4c3a-8ea2-fa901512d5c2)

ä»¥ä¸‹æ˜¯ Python çš„æ¨¡æ“¬ç’°å¢ƒç¨‹å¼ç¢¼ï¼š

```python
import numpy as np
import matplotlib.pyplot as plt

class BanditEnv:
    def __init__(self, k=30, random_seed=None):
        self.k = k
        # è¨­ç½®éš¨æ©Ÿç¨®å­ä»¥ä¿è­‰æ¯æ¬¡åŸ·è¡Œçµæœä¸€è‡´
        if random_seed is not None:
            np.random.seed(random_seed)
        # ç”ŸæˆçœŸå¯¦å›å ±ï¼Œä½¿ 0 å· arm æœ€é«˜ï¼Œé€æ¸é€’å‡åˆ°è´Ÿæ•°
        self.true_means = np.linspace(40, -40, k)

    def pull(self, arm):
        return np.random.normal(self.true_means[arm], 1)

# è¨­å®šéš¨æ©Ÿç¨®å­
random_seed = 10

# è¨­å®šç’°å¢ƒï¼Œä¸¦å›ºå®šéš¨æ©Ÿç¨®å­
env = BanditEnv(k=30, random_seed=random_seed)

# ç¹ªè£½æ¯å€‹ arm çš„çœŸå¯¦åƒ¹å€¼
plt.figure(figsize=(10, 6))
plt.bar(range(env.k), env.true_means, color='skyblue')
plt.xlabel('Arm')
plt.ylabel('True Mean Value')
plt.title(f'True Value of Each Arm (Random Seed: {random_seed})')
plt.grid(True)
plt.show()

```

---

## ğŸ¯ æ¼”ç®—æ³•ä¸€ï¼šEpsilon-Greedy

### (1) Algorithm Formula (LaTeX)

![image](https://github.com/user-attachments/assets/1e93387a-dd24-432a-b5be-ca6fbdccd743)

```latex
\documentclass{article}
\usepackage{amsmath}
\usepackage[margin=1in]{geometry}  
\usepackage{tcolorbox}  

\begin{document}

\begin{center}
    \LARGE \textbf{$\varepsilon$-Greedy Algorithm}
\end{center}

\vspace{0.5em}
\hrule
\vspace{1em}

\begin{tcolorbox}[colback=gray!5, colframe=black!40, title=Action Selection Rule]
At each time step $t$, the action $A_t$ is selected as:

\[
A_t =
\begin{cases}
\arg\max\limits_a Q_t(a) & \text{with probability } 1 - \varepsilon \\
\text{random arm} & \text{with probability } \varepsilon
\end{cases}
\]

Where:
\begin{itemize}
    \item $Q_t(a)$ is the estimated value of the action $a$ at time $t$.
    \item $\varepsilon$ is a small value (e.g. 0.1) controlling exploration.
\end{itemize}
\end{tcolorbox}

\vspace{1em}
\hrule
\vspace{1em}

\begin{tcolorbox}[colback=gray!5, colframe=black!40, title=Value Update Rule]
After choosing action $a$ and receiving reward $R_t$, the value estimate is updated as:

\[
Q_{t+1}(a) = Q_t(a) + \alpha \left( R_t - Q_t(a) \right)
\]

Here, $\alpha \in (0,1]$ is the learning rate that controls how quickly the estimates adapt to new rewards.
\end{tcolorbox}

\end{document}


```

### (2) ChatGPT Prompt

> "è«‹ç°¡è¦è§£é‡‹ epsilon-greedy æ¼”ç®—æ³•å¦‚ä½•åœ¨æ¢ç´¢èˆ‡åˆ©ç”¨ä¹‹é–“å–å¾—å¹³è¡¡ï¼Œä¸¦èˆ‰ä¾‹èªªæ˜ epsilon å€¼è®ŠåŒ–å°è¡Œç‚ºçš„å½±éŸ¿ã€‚"

### (3) ç¨‹å¼ç¢¼èˆ‡åœ–è¡¨
é€™æ˜¯ä¸€ç¨®å¹³è¡¡æ¢ç´¢èˆ‡åˆ©ç”¨çš„ç­–ç•¥ï¼š
 - ä»¥ 1% çš„æ©Ÿç‡é€²è¡Œéš¨æ©Ÿæ¢ç´¢ï¼ˆé¸æ“‡ä»»æ„ armï¼‰
 - ä»¥ 99% çš„æ©Ÿç‡é¸æ“‡ç›®å‰ä¼°è¨ˆå ±é…¬æœ€é«˜çš„ armï¼ˆåˆ©ç”¨ï¼‰

![image](https://github.com/user-attachments/assets/8f98e6a6-f06f-4d07-a765-75884b701b26)
```python
# epsilon-greedy ç­–ç•¥
def epsilon_greedy(env, epsilon=0.01, steps=10000):
    k = env.k
    Q = np.zeros(k)  # åˆå§‹åŒ–æ¯ä¸ª arm çš„ä¼°è®¡å€¼
    N = np.zeros(k)  # åˆå§‹åŒ–æ¯ä¸ª arm è¢«é€‰æ‹©çš„æ¬¡æ•°
    rewards = []
    cumulative = 0

    for t in range(steps):
        if np.random.rand() < epsilon:
            action = np.random.choice(k)  # æ¢ç´¢
        else:
            action = np.argmax(Q)  # åˆ©ç”¨

        reward = env.pull(action)
        N[action] += 1
        Q[action] += (reward - Q[action]) / N[action]  # æ›´æ–°ä¼°è®¡
        cumulative += reward
        rewards.append(cumulative)

    return rewards, N

# æ‰§è¡Œæ”¹è¿›çš„ epsilon-greedy ç­–ç•¥
eps_rewards, eps_N = epsilon_greedy(env, epsilon=0.01)

# è®¡ç®—å¹³å‡æ¯æ­¥æŠ¥é…¬
avg_rewards = [r / (i + 1) for i, r in enumerate(eps_rewards)]

# ç»˜åˆ¶å›¾è¡¨
fig, axes = plt.subplots(1, 3, figsize=(18, 5))

# 1. ç´¯ç§¯æŠ¥é…¬
axes[0].plot(eps_rewards)
axes[0].set_title('Cumulative Reward')
axes[0].set_xlabel('Steps')
axes[0].set_ylabel('Total Reward')
axes[0].grid(True)

# 2. å¹³å‡æ¯æ­¥æŠ¥é…¬
axes[1].plot(avg_rewards)
axes[1].set_title('Average Reward per Step')
axes[1].set_xlabel('Steps')
axes[1].set_ylabel('Average Reward')
axes[1].grid(True)

# 3. æ¯ä¸ª arm çš„é€‰æ‹©æ¬¡æ•°
axes[2].bar(np.arange(env.k), eps_N)
axes[2].set_title('Arm Selection Counts')
axes[2].set_xlabel('Arm')
axes[2].set_ylabel('Times Selected')
axes[2].grid(True, axis='y')

plt.tight_layout()
plt.suptitle('Epsilon-Greedy Strategy Summary', fontsize=16, y=1.05)
plt.show()
```

## (4) çµæœåˆ†æ

### â± æ™‚é–“è§’åº¦ï¼ˆTime Perspectiveï¼‰
 1. ç´¯ç©å ±é…¬ï¼ˆCumulative Rewardï¼‰
  - ç´¯ç©å ±é…¬æ›²ç·šæ•´é«”ä¸Šå‡ï¼Œä½†éç¨‹ä¸­æœ‰æ˜é¡¯æ³¢å‹•ï¼Œä¸”æˆé•·é€Ÿåº¦åæ…¢ï¼Œæœ€çµ‚ç´¯ç©å ±é…¬é ä½æ–¼å…¶ä»–ç­–ç•¥ã€‚
  - é¡¯ç¤º Epsilon-Greedy ç­–ç•¥åœ¨æœ¬æ¬¡è¨­å®šä¸­ç„¡æ³•æœ‰æ•ˆè¾¨è­˜å‡ºçœŸæ­£é«˜å ±é…¬çš„ armï¼Œå°è‡´æ¢ç´¢èˆ‡ exploitation çš„è¡¨ç¾çš†ä¸ç†æƒ³ã€‚

 2. å¹³å‡æ¯æ­¥å ±é…¬ï¼ˆAverage Reward per Stepï¼‰
  - å¹³å‡æ¯æ­¥å ±é…¬è¶¨å‹¢ç·©æ…¢ä¸Šå‡ï¼Œä½†æœ€çµ‚ç©©å®šåœ¨éå¸¸ä½çš„å€¼ï¼ˆç´„ 0.05ï¼‰ï¼Œé¡¯ç¤ºå…¶é•·æœŸæ”¶ç›Šéå¸¸æœ‰é™ã€‚
  - ç”±æ–¼æœªæ­£ç¢ºæ‰¾åˆ°æœ€ä½³ armï¼Œä½¿å¾— exploitation éšæ®µä¹Ÿç„¡æ³•æœ‰æ•ˆæå‡å¹³å‡æ”¶ç›Šã€‚
---
### ğŸ“Œ ç©ºé–“è§’åº¦ï¼ˆSpace Perspectiveï¼‰
 3. arm é¸æ“‡æ¬¡æ•¸ï¼ˆArm Selection Countsï¼‰
  - Epsilon-Greedy ç­–ç•¥ä¸‹ï¼Œç¬¬ 17 è™Ÿ arm è¢«é¸æ“‡æ¬¡æ•¸æœ€å¤šï¼Œè€ŒçœŸæ­£çš„æœ€ä½³ armï¼ˆç¬¬ 15 è™Ÿï¼‰å¹¾ä¹æ²’æœ‰è¢«é‡é»é¸å–ã€‚
  - è¡¨æ˜ Epsilon-Greedy åœ¨æœ¬æƒ…å¢ƒä¸‹çš„æ¢ç´¢æ•ˆæœä¸å¥½ï¼Œæ—©æœŸæ¢ç´¢é¸éŒ¯ armï¼Œå¾ŒçºŒ exploitation åˆå …æŒéŒ¯èª¤çš„ armï¼Œå°è‡´æ•´é«”è¡¨ç¾å—é™ã€‚
---

## ğŸ“Œ æ¼”ç®—æ³•äºŒï¼šUCB (Upper Confidence Bound)

### (1) Algorithm Formula (LaTeX)

![image](https://github.com/user-attachments/assets/0c6497c0-270d-443e-a6e2-7d7cdf7e65f7)

```latex
\documentclass{article}
\usepackage{amsmath}
\usepackage[margin=1in]{geometry} 
\usepackage{tcolorbox}  

\begin{document}

\begin{center}
    \LARGE \textbf{UCB Algorithm for Multi-Armed Bandit}
\end{center}

\vspace{0.5em}
\hrule
\vspace{1em}

\begin{tcolorbox}[colback=gray!5, colframe=black!40, title=Action Selection Rule]
At each time step $t$, the action $A_t$ is selected as:

\[
A_t = \arg\max_a \left[ Q_t(a) + c \cdot \sqrt{\frac{\ln t}{N_t(a)}} \right]
\]

Where:
\begin{itemize}
    \item $Q_t(a)$ is the estimated value of action $a$ at time $t$.
    \item $N_t(a)$ is the number of times action $a$ has been selected up to time $t$.
    \item $c > 0$ is a tunable parameter that balances exploration and exploitation.
    \item $\ln t$ encourages exploration of less-frequently selected actions.
\end{itemize}
\end{tcolorbox}

\vspace{1em}
\hrule
\vspace{1em}

\begin{tcolorbox}[colback=gray!5, colframe=black!40, title=Value Update Rule]
After choosing action $a$ and receiving reward $R_t$, the value estimate is updated as:

\[
Q_{t+1}(a) = Q_t(a) + \alpha \left( R_t - Q_t(a) \right)
\]

Here, $\alpha \in (0,1]$ is the learning rate that controls how quickly the estimates adapt to new rewards. Alternatively, one can use a sample average (i.e., $\alpha = \frac{1}{N_t(a)}$) for non-stationary problems.
\end{tcolorbox}

\end{document}

```
### (2) ChatGPT Prompt

> "è«‹èªªæ˜ UCB æ¼”ç®—æ³•å¦‚ä½•é€éç½®ä¿¡å€é–“é”åˆ°æ¢ç´¢èˆ‡åˆ©ç”¨çš„å¹³è¡¡ï¼Œä¸¦æŒ‡å‡ºå¸¸æ•¸ c å°è¡Œç‚ºçš„å½±éŸ¿ã€‚"

### (3) ç¨‹å¼ç¢¼èˆ‡åœ–è¡¨
![image](https://github.com/user-attachments/assets/3437728f-7a24-47f4-9ab1-7fcad5f7c214)
```python
# UCB ç­–ç•¥
def ucb(env, c=3, steps=10000):
    k = env.k
    Q = np.zeros(k)  # æ¯å€‹ arm çš„ä¼°ç®—æœŸæœ›å ±é…¬
    N = np.zeros(k)  # æ¯å€‹ arm è¢«é¸æ“‡çš„æ¬¡æ•¸
    rewards = []
    cumulative = 0

    for t in range(1, steps + 1):
        ucb_values = Q + c * np.sqrt(np.log(t) / (N + 1e-6))  # è¨ˆç®— UCB å€¼ï¼Œé¿å…é™¤ä»¥é›¶
        # éš¨æ©Ÿé¸æ“‡å…·æœ‰æœ€å¤§ UCB å€¼çš„ arm
        max_ucb_value = np.max(ucb_values)
        best_arms = np.where(ucb_values == max_ucb_value)[0]
        action = np.random.choice(best_arms)  # éš¨æ©Ÿé¸æ“‡å…¶ä¸­ä¸€å€‹æœ€ä½³ arm
        
        reward = env.pull(action)

        N[action] += 1
        Q[action] += (reward - Q[action]) / N[action]  # æ›´æ–° Q å€¼
        cumulative += reward
        rewards.append(cumulative)

    return rewards, N

# æ‰§è¡Œ UCB ç­–ç•¥
ucb_rewards, ucb_N = ucb(env)

# è®¡ç®—å¹³å‡æ¯æ­¥æŠ¥é…¬
avg_rewards = [r / (i + 1) for i, r in enumerate(ucb_rewards)]

# ç»˜åˆ¶å›¾è¡¨
fig, axes = plt.subplots(1, 3, figsize=(18, 5))

# 1. ç´¯ç§¯æŠ¥é…¬
axes[0].plot(ucb_rewards)
axes[0].set_title('Cumulative Reward')
axes[0].set_xlabel('Steps')
axes[0].set_ylabel('Total Reward')
axes[0].grid(True)

# 2. å¹³å‡æ¯æ­¥æŠ¥é…¬
axes[1].plot(avg_rewards)
axes[1].set_title('Average Reward per Step')
axes[1].set_xlabel('Steps')
axes[1].set_ylabel('Average Reward')
axes[1].grid(True)

# 3. æ¯ä¸ª arm çš„é€‰æ‹©æ¬¡æ•°
axes[2].bar(np.arange(env.k), ucb_N)
axes[2].set_title('Arm Selection Counts')
axes[2].set_xlabel('Arm')
axes[2].set_ylabel('Times Selected')
axes[2].grid(True, axis='y')

plt.tight_layout()
plt.suptitle('UCB Strategy Summary', fontsize=16, y=1.05)
plt.show()
```
## (4) çµæœåˆ†æ

### â± æ™‚é–“è§’åº¦ï¼ˆTime Perspectiveï¼‰
 1. ç´¯ç©å ±é…¬ï¼ˆCumulative Rewardï¼‰
  - ç´¯ç©å ±é…¬æ›²ç·šä¸Šå‡è¶¨å‹¢æ¯” Epsilon-Greedyæ›´ç©©å®šä¸”æ˜é¡¯ï¼Œé›–ç„¶ä»æœ‰äº›å°å¹…åº¦æ³¢å‹•ï¼Œä½†æ•´é«”èµ°å‹¢è‰¯å¥½ã€‚
  - èªªæ˜ UCB èƒ½å¤ åœ¨åˆæœŸå¿«é€Ÿè¾¨è­˜è¼ƒå¥½çš„ armsï¼Œä¸¦ä¸”åœ¨ exploitation éšæ®µæœ‰æ•ˆç´¯ç©å ±é…¬ã€‚

 2. å¹³å‡æ¯æ­¥å ±é…¬ï¼ˆAverage Reward per Stepï¼‰
  - å¹³å‡æ¯æ­¥å ±é…¬é€æ¼¸è¶¨æ–¼ç©©å®šï¼Œå¤§ç´„æ”¶æ–‚åˆ° 0.04ã€‚
  - æ›²ç·šç›¸å°å¹³æ»‘ï¼Œæ³¢å‹•å°ï¼Œèªªæ˜ UCB èƒ½åœ¨æ™‚é–“ä¸Šæœ‰æ•ˆåˆ©ç”¨ç¶“é©—ï¼Œæ¸›å°‘ç„¡æ•ˆæ¢ç´¢ã€‚
---
### ğŸ“Œ ç©ºé–“è§’åº¦ï¼ˆSpace Perspectiveï¼‰
 3. arm é¸æ“‡æ¬¡æ•¸ï¼ˆArm Selection Countsï¼‰
  - ç¬¬ 15 è™Ÿ arm è¢«é¸æ“‡æœ€å¤šï¼Œè­‰æ˜æˆåŠŸæ‰¾åˆ°æœ€ä½³ armã€‚
  - ä½†å…¶ä»– arm çš„é¸æ“‡æ¬¡æ•¸è¼ƒåˆ†æ•£ï¼Œé¡¯ç¤º Softmax ç¶­æŒä¸€å®šç¨‹åº¦çš„éš¨æ©Ÿæ¢ç´¢ã€‚
---

---

## ğŸ² æ¼”ç®—æ³•ä¸‰ï¼šSoftmax

### (1) Algorithm Formula (LaTeX)

![image](https://github.com/user-attachments/assets/665bcca3-6b49-4c48-a4b5-604a8857b183)

```latex
\documentclass{article}
\usepackage{amsmath}
\usepackage[margin=1in]{geometry}  % æ›´å¥½çœ‹ä¸€ç‚¹çš„é¡µé¢è¾¹è·
\usepackage{tcolorbox}  % ç”¨äºæ¼‚äº®çš„å†…å®¹æ¡†

\begin{document}

\begin{center}
    \LARGE \textbf{Softmax Algorithm for Multi-Armed Bandit}
\end{center}

\vspace{0.5em}
\hrule
\vspace{1em}

\begin{tcolorbox}[colback=gray!5, colframe=black!40, title=Action Selection Rule]
At each time step $t$, the probability of selecting action $a$ is computed using the softmax function:

\[
P_t(a) = \frac{e^{Q_t(a)/\tau}}{\sum\limits_b e^{Q_t(b)/\tau}}
\]

Then, action $A_t$ is sampled according to the distribution $P_t(a)$.

Where:
\begin{itemize}
    \item $Q_t(a)$ is the estimated value of action $a$ at time $t$.
    \item $\tau > 0$ is the \textbf{temperature} parameter that controls the randomness of action selection:
    \begin{itemize}
        \item High $\tau$: more exploration (actions have similar probabilities)
        \item Low $\tau$: more exploitation (focuses on high-value actions)
    \end{itemize}
\end{itemize}
\end{tcolorbox}

\vspace{1em}
\hrule
\vspace{1em}

\begin{tcolorbox}[colback=gray!5, colframe=black!40, title=Value Update Rule]
After choosing action $a$ and receiving reward $R_t$, the value estimate is updated as:

\[
Q_{t+1}(a) = Q_t(a) + \alpha \left( R_t - Q_t(a) \right)
\]

Here, $\alpha \in (0,1]$ is the learning rate that determines how quickly the estimate adapts to new information. A decaying or sample-average approach to $\alpha$ is also commonly used.
\end{tcolorbox}

\end{document}

```

### (2) ChatGPT Prompt

> "è«‹ç”¨ç°¡å–®ä¾‹å­èªªæ˜ Softmax ç­–ç•¥å¦‚ä½•æ ¹æ“šä¸åŒæº«åº¦åƒæ•¸ï¼ˆ\tauï¼‰èª¿æ•´æ¢ç´¢ç¨‹åº¦ï¼Œä¸¦è§£é‡‹ç‚ºä½• \tau è¶Šå¤§è¶Šå‚¾å‘æ¢ç´¢ã€‚"

### (3) ç¨‹å¼ç¢¼èˆ‡åœ–è¡¨
![image](https://github.com/user-attachments/assets/1f26b984-02a8-42c1-bf7b-6d7a5861c1e5)
```python
# Softmax ç­–ç•¥
def softmax(env, tau=0.2, steps=10000):
    k = env.k
    Q = np.zeros(k)  # æ¯å€‹ arm çš„ä¼°ç®—æœŸæœ›å ±é…¬
    N = np.zeros(k)  # æ¯å€‹ arm è¢«é¸æ“‡çš„æ¬¡æ•¸
    rewards = []
    cumulative = 0

    for t in range(steps):
        exp_Q = np.exp(Q / tau)  # è¨ˆç®— Q å€¼çš„ Softmax æ©Ÿç‡
        probs = exp_Q / np.sum(exp_Q)  # æ©Ÿç‡æ­£è¦åŒ–
        action = np.random.choice(k, p=probs)  # æ ¹æ“šæ©Ÿç‡é¸æ“‡ arm

        reward = env.pull(action)
        N[action] += 1
        Q[action] += (reward - Q[action]) / N[action]
        cumulative += reward
        rewards.append(cumulative)

    return rewards, N

# æ‰§è¡Œ Softmax ç­–ç•¥
softmax_rewards, softmax_N = softmax(env)

# è®¡ç®—å¹³å‡æ¯æ­¥æŠ¥é…¬
avg_rewards = [r / (i + 1) for i, r in enumerate(softmax_rewards)]

# ç»˜åˆ¶å›¾è¡¨
fig, axes = plt.subplots(1, 3, figsize=(18, 5))

# 1. ç´¯ç§¯æŠ¥é…¬
axes[0].plot(softmax_rewards)
axes[0].set_title('Cumulative Reward')
axes[0].set_xlabel('Steps')
axes[0].set_ylabel('Total Reward')
axes[0].grid(True)

# 2. å¹³å‡æ¯æ­¥æŠ¥é…¬
axes[1].plot(avg_rewards)
axes[1].set_title('Average Reward per Step')
axes[1].set_xlabel('Steps')
axes[1].set_ylabel('Average Reward')
axes[1].grid(True)

# 3. æ¯ä¸ª arm çš„é€‰æ‹©æ¬¡æ•°
axes[2].bar(np.arange(env.k), softmax_N)
axes[2].set_title('Arm Selection Counts')
axes[2].set_xlabel('Arm')
axes[2].set_ylabel('Times Selected')
axes[2].grid(True, axis='y')

plt.tight_layout()
plt.suptitle('Softmax Strategy Summary', fontsize=16, y=1.05)
plt.show()
```
![image](https://github.com/user-attachments/assets/e9aa5448-1206-4d6a-990b-bcec5ba811d9)

## (4) çµæœåˆ†æ

### â± æ™‚é–“è§’åº¦ï¼ˆTime Perspectiveï¼‰
 1. ç´¯ç©å ±é…¬ï¼ˆCumulative Rewardï¼‰
  - ç´¯ç©å ±é…¬æ›²ç·šä¸Šå‡è¶¨å‹¢æ¯” UCB å’Œ Epsilon-Greedyæ›´ç©©å®šä¸”æ˜é¡¯ï¼Œé›–ç„¶ä»æœ‰äº›å°å¹…åº¦æ³¢å‹•ï¼Œä½†æ•´é«”èµ°å‹¢è‰¯å¥½ã€‚
  - èªªæ˜ Softmax èƒ½å¤ åœ¨åˆæœŸå¿«é€Ÿè¾¨è­˜è¼ƒå¥½çš„ armsï¼Œä¸¦ä¸”åœ¨ exploitation éšæ®µæœ‰æ•ˆç´¯ç©å ±é…¬ã€‚

 2. å¹³å‡æ¯æ­¥å ±é…¬ï¼ˆAverage Reward per Stepï¼‰
  - å¹³å‡æ¯æ­¥å ±é…¬é€æ¼¸è¶¨æ–¼ç©©å®šï¼Œå¤§ç´„æ”¶æ–‚åˆ° 0.05ã€‚
  - é•·æœŸè¡¨ç¾ç©©å¥ï¼Œç•¥å„ªæ–¼ UCBã€‚
---
### ğŸ“Œ ç©ºé–“è§’åº¦ï¼ˆSpace Perspectiveï¼‰
 3. arm é¸æ“‡æ¬¡æ•¸ï¼ˆArm Selection Countsï¼‰
  - é›–ç„¶ç¬¬ 15 è™Ÿ armä»ç„¶æ˜¯è¢«æœ€å¤šæ¬¡é¸æ“‡çš„ï¼Œä½†å…¶ä»– arm ä¹Ÿæœ‰è¼ƒå¤šæ¬¡çš„æ¢ç´¢ç´€éŒ„ï¼Œç›¸æ¯” Epsilon-Greedyæ›´åŠ å¹³å‡ã€‚
  - é¡¯ç¤º UCB ä¿æŒäº†ä¸€å®šç¨‹åº¦çš„æ¢ç´¢ï¼ŒåŒæ™‚ä¹Ÿèƒ½èšç„¦åœ¨æœ€ä½³ armï¼Œç¬¦åˆå…¶ã€Œæ¨‚è§€åˆå§‹ä¼°è¨ˆã€çš„ç‰¹æ€§ã€‚

---

## ğŸ¯ æ¼”ç®—æ³•å››ï¼šThompson Sampling

### (1) Algorithm Formula (LaTeX)

![image](https://github.com/user-attachments/assets/9ca620c3-da84-4243-b4f6-b33f1742bfa8)


```latex
\documentclass{article}
\usepackage{amsmath}
\usepackage[margin=1in]{geometry}  % æ›´å¥½çœ‹ä¸€ç‚¹çš„é¡µé¢è¾¹è·
\usepackage{tcolorbox}  % ç”¨äºæ¼‚äº®çš„å†…å®¹æ¡†

\begin{document}

\begin{center}
    \LARGE \textbf{Thompson Sampling for Multi-Armed Bandit}
\end{center}

\vspace{0.5em}
\hrule
\vspace{1em}

\begin{tcolorbox}[colback=gray!5, colframe=black!40, title=Action Selection Rule]
At each time step $t$, the agent maintains a posterior distribution over the reward of each action. The action $A_t$ is selected as:

\[
A_t = \arg\max_a \theta_a
\]

Where:
\begin{itemize}
    \item For each action $a$, sample $\theta_a$ from its posterior distribution $P(\theta_a \mid \text{data})$.
    \item Choose the action $a$ with the highest sampled $\theta_a$.
\end{itemize}

\textbf{Typical case: Bernoulli rewards}

\begin{itemize}
    \item Use Beta distribution as the conjugate prior.
    \item Maintain parameters $(\alpha_a, \beta_a)$ for each action $a$:
    \[
    \theta_a \sim \text{Beta}(\alpha_a, \beta_a)
    \]
\end{itemize}
\end{tcolorbox}

\vspace{1em}
\hrule
\vspace{1em}

\begin{tcolorbox}[colback=gray!5, colframe=black!40, title=Posterior Update Rule]
After choosing action $a$ and observing reward $R_t \in \{0,1\}$ (e.g., Bernoulli reward), update the Beta distribution parameters:

\[
\alpha_a \leftarrow \alpha_a + R_t, \quad \beta_a \leftarrow \beta_a + (1 - R_t)
\]

This keeps the posterior up to date:
\[
\theta_a \sim \text{Beta}(\alpha_a, \beta_a)
\]
\end{tcolorbox}

\end{document}

```

### (2) ChatGPT Prompt

> "è«‹ç”¨ç°¡å–®èªè¨€èªªæ˜ç‚ºä½• Thompson Sampling èƒ½è‡ªç„¶åœ°åœ¨æ¢ç´¢èˆ‡åˆ©ç”¨é–“å–å¾—å¹³è¡¡ï¼Œä»¥åŠ beta åˆ†å¸ƒåœ¨é€™è£¡çš„æ„ç¾©ã€‚"

### (3) ç¨‹å¼ç¢¼èˆ‡åœ–è¡¨
![image](https://github.com/user-attachments/assets/dafa5402-9aea-48a3-8be4-3afadf4e488c)

```python
def thompson_sampling(env, steps=10000):
    k = env.k
    alpha = np.ones(k)
    beta = np.ones(k)
    rewards = []
    cumulative = 0
    N = np.zeros(k)  # è¨˜éŒ„æ¯å€‹ arm è¢«é¸æ“‡çš„æ¬¡æ•¸

    for t in range(steps):
        theta = np.random.beta(alpha, beta)  # æ ¹æ“š beta åˆ†ä½ˆé€²è¡ŒæŠ½æ¨£
        action = np.argmax(theta)  # é¸æ“‡æœŸæœ›å ±é…¬æœ€å¤§çš„ arm
        reward = env.pull(action)

        # å°‡å¸¸æ…‹åˆ†ä½ˆçš„å ±é…¬è½‰æ›ç‚ºäºŒå…ƒå›å ±
        reward_bin = 1 if reward > 0 else 0  

        # æ›´æ–° Beta åˆ†ä½ˆåƒæ•¸
        alpha[action] += reward_bin
        beta[action] += 1 - reward_bin
        cumulative += reward
        rewards.append(cumulative)
        N[action] += 1

    return rewards, N

# æ‰§è¡Œ Thompson Sampling ç­–ç•¥
ts_rewards, ts_N = thompson_sampling(env)

# è®¡ç®—å¹³å‡æ¯æ­¥æŠ¥é…¬
avg_rewards = [r / (i + 1) for i, r in enumerate(ts_rewards)]

# ç»˜åˆ¶å›¾è¡¨
fig, axes = plt.subplots(1, 3, figsize=(18, 5))

# 1. ç´¯ç§¯æŠ¥é…¬
axes[0].plot(ts_rewards)
axes[0].set_title('Cumulative Reward')
axes[0].set_xlabel('Steps')
axes[0].set_ylabel('Total Reward')
axes[0].grid(True)

# 2. å¹³å‡æ¯æ­¥æŠ¥é…¬
axes[1].plot(avg_rewards)
axes[1].set_title('Average Reward per Step')
axes[1].set_xlabel('Steps')
axes[1].set_ylabel('Average Reward')
axes[1].grid(True)

# 3. æ¯ä¸ª arm çš„é€‰æ‹©æ¬¡æ•°
axes[2].bar(np.arange(env.k), ts_N)
axes[2].set_title('Arm Selection Counts')
axes[2].set_xlabel('Arm')
axes[2].set_ylabel('Times Selected')
axes[2].grid(True, axis='y')

plt.tight_layout()
plt.suptitle('Thompson Sampling Strategy Summary', fontsize=16, y=1.05)
plt.show()
```
## (4) çµæœåˆ†æ

### â± æ™‚é–“è§’åº¦ï¼ˆTime Perspectiveï¼‰
 1. ç´¯ç©å ±é…¬ï¼ˆCumulative Rewardï¼‰
  - ç´¯ç©å ±é…¬æ›²ç·šæœ€ç‚ºé™¡å³­ä¸”ç©©å®šä¸Šå‡ï¼Œæœ€çµ‚ç´¯ç©å ±é…¬çªç ´ 1000ã€‚
  - é¡¯è‘—å„ªæ–¼å…¶ä»–ä¸‰ç¨®ç­–ç•¥ï¼Œè¡¨ç¾æœ€ä½³ã€‚

 2. å¹³å‡æ¯æ­¥å ±é…¬ï¼ˆAverage Reward per Stepï¼‰
  - å¹³å‡æ¯æ­¥å ±é…¬å¿«é€Ÿæå‡ä¸¦ç©©å®šåœ¨ç´„ 0.1 å·¦å³ï¼Œé é«˜æ–¼å…¶ä»–æ–¹æ³•ã€‚
  - èªªæ˜ Thompson Sampling æ—¢èƒ½å¿«é€Ÿæ‰¾åˆ°æœ€ä½³ armï¼Œåˆèƒ½ç©©å®š exploitationï¼Œé•·æœŸæ”¶ç›Šæ¥µä½³ã€‚
---
### ğŸ“Œ ç©ºé–“è§’åº¦ï¼ˆSpace Perspectiveï¼‰
 3. arm é¸æ“‡æ¬¡æ•¸ï¼ˆArm Selection Countsï¼‰
  - ç¬¬ 15 è™Ÿ arm è¢«æ¥µå¤§æ¯”ä¾‹åœ°é¸æ“‡ï¼Œå¹¾ä¹å£Ÿæ–·äº†æ‰€æœ‰æ“ä½œã€‚
  - å…¶ä»– arm é¸æ“‡æ¬¡æ•¸æ¥µå°‘ï¼Œé¡¯ç¤º Thompson Sampling åœ¨ç¢ºå®šæœ€ä½³ arm å¾Œè¿…é€Ÿé›†ä¸­ exploitationã€‚
---

---

## ğŸ“Š æ‰€æœ‰æ¼”ç®—æ³•æ¯”è¼ƒåœ–è¡¨
![image](https://github.com/user-attachments/assets/0e4d5069-fa62-42b4-aa4a-40cc390504d8)
![image](https://github.com/user-attachments/assets/a538a4af-ab99-4ef1-8297-1bf1d29ef187)

```python
env = BanditEnv()
eps = epsilon_greedy(env)
env = BanditEnv()
ucb_ = ucb(env)
env = BanditEnv()
soft = softmax(env)
env = BanditEnv()
ts = thompson_sampling(env)

plt.figure(figsize=(10,6))
plt.plot(eps, label='Epsilon-Greedy')
plt.plot(ucb_, label='UCB')
plt.plot(soft, label='Softmax')
plt.plot(ts, label='Thompson Sampling')
plt.xlabel('Steps')
plt.ylabel('Cumulative Reward')
plt.title('Algorithm Comparison: Cumulative Reward')
plt.legend()
plt.grid(True)
plt.show()
```
# ğŸ“‹ ç¸½çµæ¯”è¼ƒè¡¨ï¼ˆæ™‚é–“åˆ†æ + ç©ºé–“åˆ†æï¼‰

| ç­–ç•¥ | ç´¯ç©å ±é…¬æˆé•· | å¹³å‡æ¯æ­¥å ±é…¬ | æ˜¯å¦æˆåŠŸæ‰¾åˆ°æœ€ä½³ Armï¼ˆç¬¬15å€‹ï¼‰ | Arm é¸æ“‡åˆ†ä½ˆç‰¹æ€§ |
|:-----|:--------------|:------------|:----------------------------|:----------------|
| **Epsilon-Greedy** | ç·©æ…¢ä¸”éœ‡ç›ª | ç´„ 0.02ï¼Œåä½ | âŒ æ²’æœ‰æ‰¾åˆ°ï¼ˆé›†ä¸­åœ¨ç¬¬20å€‹ armï¼‰ | å–®ä¸€ suboptimal arm è¢«éåº¦é¸æ“‡ |
| **UCB** | ç©©å®šä¸”æˆé•·å¿« | ç´„ 0.04ï¼Œç©©å®šä¸­ç­‰ | âœ… æˆåŠŸæ‰¾åˆ°æœ€ä½³ arm | ä¸»é¸æœ€ä½³ armï¼Œä½†ä»æŒçºŒå°‘é‡æ¢ç´¢å…¶ä»– |
| **Softmax** | å¹³ç©©ä¸Šå‡ | ç´„ 0.05ï¼Œç•¥é«˜æ–¼ UCB | âœ… æˆåŠŸæ‰¾åˆ°æœ€ä½³ arm | ä¸»è¦é›†ä¸­åœ¨æœ€ä½³ armï¼Œä½†ä»æœ‰åˆ†æ•£æ¢ç´¢ |
| **Thompson Sampling** | æ¥µé€Ÿç©©å®šä¸Šå‡ | ç´„ 0.1ï¼Œæœ€é«˜ | âœ… æˆåŠŸä¸”å¿«é€Ÿæ‰¾åˆ°æœ€ä½³ arm | å¹¾ä¹å®Œå…¨é›†ä¸­åœ¨æœ€ä½³ armï¼Œæ¥µå°‘æ¢ç´¢å…¶ä»– |

---

# ğŸ“ˆ çµè«–èˆ‡å·®ç•°æ¯”è¼ƒè¡¨

| é …ç›® | Epsilon-Greedy | UCB | Softmax | Thompson Sampling |
|:-----|:---------------|:----|:--------|:------------------|
| **æ¢ç´¢æ•ˆç‡** | ä½ï¼Œå®¹æ˜“é¸éŒ¯ | ä¸­é«˜ï¼Œç†æ€§æ¢ç´¢ | ä¸­ï¼Œå¹³æ»‘æ¢ç´¢ | é«˜ï¼Œå¿«é€Ÿèšç„¦ |
| **Exploitation æ•ˆç‡** | ä½ | ä¸­ç­‰åé«˜ | é«˜ | æ¥µé«˜ |
| **æ”¶æ–‚é€Ÿåº¦** | æ…¢ä¸”ä¸ç©©å®š | ç©©å®šæ”¶æ–‚ | ç©©å®šä¸”è¼ƒå¿« | éå¸¸å¿« |
| **ç´¯ç©å ±é…¬æœ€çµ‚è¡¨ç¾** | æœ€ä½ | ä¸­ç­‰ | æ¬¡é«˜ | æœ€é«˜ |
| **ç©©å¥æ€§** | å·® | å¥½ | ä¸­ç­‰åå¥½ | éå¸¸å¥½ |

---

# ğŸ§  å„æ¼”ç®—æ³•å„ªåŠ£èˆ‡é©ç”¨æƒ…å¢ƒæ¯”è¼ƒè¡¨

| ç­–ç•¥ | å„ªé» | ç¼ºé» | é©ç”¨æƒ…å¢ƒ |
|:-----|:-----|:-----|:---------|
| **Epsilon-Greedy** | ç°¡å–®æ˜“æ‡‚ï¼Œå¯¦ä½œå¿«é€Ÿ | å®¹æ˜“å¡åœ¨éŒ¯èª¤ armï¼Œé•·æœŸè¡¨ç¾å·® | å•é¡Œç°¡å–®ã€arm æ•¸é‡å°‘ã€ä¸è¦æ±‚é«˜ç²¾åº¦æ™‚ |
| **UCB** | ç†è«–åŸºç¤å¼·ï¼Œè‡ªå‹•å¹³è¡¡æ¢ç´¢èˆ‡åˆ©ç”¨ | åˆæœŸéœ€è¦è¼ƒå¤šæ¢ç´¢æ­¥é©Ÿï¼Œæ”¶æ–‚é€Ÿåº¦å—é™ | å•é¡Œä¸­ arm æ•¸é‡ä¸­ç­‰ï¼Œéœ€è¦å¹³ç©©æˆé•·çš„æƒ…å¢ƒ |
| **Softmax** | æ¢ç´¢æ›´å¹³æ»‘ï¼Œé¿å…éæ—©æ”¶æ–‚ | éœ€è¦å¾®èª¿æº«åº¦åƒæ•¸ï¼Œå¦å‰‡æ¢ç´¢ä¸è¶³æˆ–éåº¦ | éœ€å…¼é¡§ç©©å¥èˆ‡éˆæ´»æ€§çš„ä¸­å‹å°ˆæ¡ˆ |
| **Thompson Sampling** | æ¢ç´¢èˆ‡ exploitation è‡ªç„¶èåˆï¼Œæ”¶æ–‚å¿« | å¯¦ä½œä¸Šç¨å¾®è¤‡é›œï¼Œä¾è³´éš¨æ©Ÿæ€§ | è³‡æºæœ‰é™æ™‚ã€éœ€è¦å¿«é€Ÿæ±ºç­–çš„å ´æ™¯ï¼ˆå¦‚ç·šä¸Šå»£å‘ŠæŠ•æ”¾ã€æ¨è–¦ç³»çµ±ï¼‰ |

---

# ğŸ“ ç¸½çµå°çµè«–

- **Epsilon-Greedy**ï¼šåœ¨æœ¬æƒ…å¢ƒè¡¨ç¾æœ€å·®ï¼Œæ¢ç´¢éç¨‹å¤±æ•—å°è‡´ exploitation éšæ®µç„¡æ³•å½Œè£œã€‚
- **UCB**ï¼šåˆç†æ‰¾åˆ°æœ€ä½³ armï¼Œç©©å®šæˆé•·ï¼Œä½†é•·æœŸæ”¶ç›Šç•¥éœã€‚
- **Softmax**ï¼šæ‰¾åˆ°æœ€ä½³ armï¼Œé•·æœŸè¡¨ç¾è‰¯å¥½ï¼Œä½†éœ€è¦å°å¿ƒåƒæ•¸è¨­å®šã€‚
- **Thompson Sampling**ï¼šè¡¨ç¾æœ€ä½³ï¼Œå¿«é€Ÿä¸”æœ‰æ•ˆåœ°æŒæ¡æœ€ä½³ armï¼Œæœ€é©åˆè³‡æºæœ‰é™ä¸”è¦æ±‚å¿«é€Ÿå­¸ç¿’çš„æƒ…å¢ƒã€‚
