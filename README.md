![image](https://github.com/user-attachments/assets/62e7f50c-9b5d-41a5-8344-97552307c9ac)# Multi-Armed-Bandit-MAB

# HW3: Explore and Exploit for Arm-Bandit Problem

## ğŸ® ç’°å¢ƒä»‹ç´¹ï¼šå¤šè‡‚æ©Ÿå™¨äººï¼ˆMulti-Armed Banditï¼‰

æœ¬ä½œæ¥­ä¸­ï¼Œæˆ‘å€‘æ¨¡æ“¬ä¸€å€‹åŒ…å« k è‡‚çš„æ‹‰éœ¸æ©Ÿï¼Œæ¯å€‹è‡‚åœ¨æ¯æ¬¡æ‹‰ä¸‹æ™‚æœƒæ ¹æ“šå›ºå®šçš„åˆ†å¸ƒçµ¦å‡ºä¸åŒçš„å ±é…¬ã€‚é€™æ˜¯ä¸€å€‹æ¢ç´¢ï¼ˆexplorationï¼‰èˆ‡åˆ©ç”¨ï¼ˆexploitationï¼‰å…¼å…·çš„ç¶“å…¸å•é¡Œï¼Œéœ€æ‰¾å‡ºåœ¨æœ‰é™è©¦é©—æ¬¡æ•¸ä¸‹æœ€å¤§åŒ–ç´¯ç©çå‹µçš„ç­–ç•¥ã€‚

ä»¥ä¸‹æ˜¯ Python çš„æ¨¡æ“¬ç’°å¢ƒç¨‹å¼ç¢¼ï¼š

```python
import numpy as np

class BanditEnv:
    def __init__(self, k=10):
        self.k = k
        self.true_means = np.random.normal(0, 1, k)
    
    def pull(self, arm):
        return np.random.normal(self.true_means[arm], 1)
```

---

## ğŸ¯ æ¼”ç®—æ³•ä¸€ï¼šEpsilon-Greedy

### (1) Algorithm Formula (LaTeX)

\[
A_t = \begin{cases}
\arg\max_a Q_t(a) & \text{with probability } 1 - \epsilon \\
a \sim \text{Uniform}(0, k-1) & \text{with probability } \epsilon
\end{cases}
\]

```latex
A_t = \begin{cases}
\arg\max_a Q_t(a) & \text{with probability } 1 - \epsilon \\
a \sim \text{Uniform}(0, k-1) & \text{with probability } \epsilon
\end{cases}
```

### (2) ChatGPT Prompt

> "è«‹ç°¡è¦è§£é‡‹ epsilon-greedy æ¼”ç®—æ³•å¦‚ä½•åœ¨æ¢ç´¢èˆ‡åˆ©ç”¨ä¹‹é–“å–å¾—å¹³è¡¡ï¼Œä¸¦èˆ‰ä¾‹èªªæ˜ epsilon å€¼è®ŠåŒ–å°è¡Œç‚ºçš„å½±éŸ¿ã€‚"

### (3) ç¨‹å¼ç¢¼èˆ‡åœ–è¡¨

```python
import matplotlib.pyplot as plt

def epsilon_greedy(env, epsilon=0.1, steps=1000):
    k = env.k
    Q = np.zeros(k)
    N = np.zeros(k)
    rewards = []
    cumulative = 0

    for t in range(steps):
        if np.random.rand() < epsilon:
            action = np.random.choice(k)
        else:
            action = np.argmax(Q)
        
        reward = env.pull(action)
        N[action] += 1
        Q[action] += (reward - Q[action]) / N[action]
        cumulative += reward
        rewards.append(cumulative)
    
    return rewards

env = BanditEnv()
eps_rewards = epsilon_greedy(env)

plt.plot(eps_rewards, label='Epsilon-Greedy')
plt.xlabel('Steps')
plt.ylabel('Cumulative Reward')
plt.title('Epsilon-Greedy Performance')
plt.legend()
plt.grid(True)
plt.show()
```
![image](https://github.com/user-attachments/assets/e5210a9e-e987-45ef-be65-263dc2005d0a)

### (4) çµæœåˆ†æ

- **æ™‚é–“åˆ†æï¼š** æ”¶æ–‚é€Ÿåº¦ä¾è³´æ–¼ epsilon å€¼ï¼›è¼ƒé«˜ epsilon å°è‡´é•·æœŸå­¸ç¿’æ›´æ…¢ã€‚
- **ç©ºé–“åˆ†æï¼š** åƒ…éœ€è¨˜éŒ„æ¯å€‹ arm çš„ Q å€¼èˆ‡é¸æ“‡æ¬¡æ•¸ï¼Œç©ºé–“è¤‡é›œåº¦ç‚º \( O(k) \)ã€‚

---

## ğŸ“Œ æ¼”ç®—æ³•äºŒï¼šUCB (Upper Confidence Bound)

### (1) Algorithm Formula (LaTeX)

\[
A_t = \arg\max_a \left[ Q_t(a) + c \cdot \sqrt{\frac{\ln t}{N_t(a)}} \right]
\]

```latex
A_t = \arg\max_a \left[ Q_t(a) + c \cdot \sqrt{\frac{\ln t}{N_t(a)}} \right]
```

### (2) ChatGPT Prompt

> "è«‹èªªæ˜ UCB æ¼”ç®—æ³•å¦‚ä½•é€éç½®ä¿¡å€é–“é”åˆ°æ¢ç´¢èˆ‡åˆ©ç”¨çš„å¹³è¡¡ï¼Œä¸¦æŒ‡å‡ºå¸¸æ•¸ c å°è¡Œç‚ºçš„å½±éŸ¿ã€‚"

### (3) ç¨‹å¼ç¢¼èˆ‡åœ–è¡¨

```python
def ucb(env, c=2, steps=1000):
    k = env.k
    Q = np.zeros(k)
    N = np.zeros(k)
    rewards = []
    cumulative = 0

    for t in range(1, steps + 1):
        ucb_values = np.where(N > 0, Q + c * np.sqrt(np.log(t) / N), float('inf'))
        action = np.argmax(ucb_values)
        reward = env.pull(action)

        N[action] += 1
        Q[action] += (reward - Q[action]) / N[action]
        cumulative += reward
        rewards.append(cumulative)
    
    return rewards

env = BanditEnv()
ucb_rewards = ucb(env)

plt.plot(ucb_rewards, label='UCB')
plt.xlabel('Steps')
plt.ylabel('Cumulative Reward')
plt.title('UCB Performance')
plt.legend()
plt.grid(True)
plt.show()

```
![image](https://github.com/user-attachments/assets/0085c65d-3f4a-4b98-8021-2fd81cd55f0f)

### (4) çµæœåˆ†æ

- **æ™‚é–“åˆ†æï¼š** åˆæœŸæ¢ç´¢æ•ˆæœä½³ï¼Œæ”¶æ–‚é€Ÿåº¦å¿«ã€‚
- **ç©ºé–“åˆ†æï¼š** èˆ‡ epsilon-greedy ç›¸åŒï¼Œè¨ˆç®—æˆæœ¬å¢åŠ ã€‚

---

## ğŸ² æ¼”ç®—æ³•ä¸‰ï¼šSoftmax

### (1) Algorithm Formula (LaTeX)

\[
P(a) = \frac{\exp(Q_t(a) / \tau)}{\sum_{b=1}^{k} \exp(Q_t(b) / \tau)}
\]

```latex
P(a) = \frac{\exp(Q_t(a) / \tau)}{\sum_{b=1}^{k} \exp(Q_t(b) / \tau)}
```

### (2) ChatGPT Prompt

> "è«‹ç”¨ç°¡å–®ä¾‹å­èªªæ˜ Softmax ç­–ç•¥å¦‚ä½•æ ¹æ“šä¸åŒæº«åº¦åƒæ•¸ï¼ˆ\tauï¼‰èª¿æ•´æ¢ç´¢ç¨‹åº¦ï¼Œä¸¦è§£é‡‹ç‚ºä½• \tau è¶Šå¤§è¶Šå‚¾å‘æ¢ç´¢ã€‚"

### (3) ç¨‹å¼ç¢¼èˆ‡åœ–è¡¨

```python
def softmax(env, tau=0.1, steps=1000):
    k = env.k
    Q = np.zeros(k)
    N = np.zeros(k)
    rewards = []
    cumulative = 0

    for t in range(steps):
        exp_Q = np.exp(Q / tau)
        probs = exp_Q / np.sum(exp_Q)
        action = np.random.choice(k, p=probs)
        
        reward = env.pull(action)
        N[action] += 1
        Q[action] += (reward - Q[action]) / N[action]
        cumulative += reward
        rewards.append(cumulative)
    
    return rewards

env = BanditEnv()
softmax_rewards = softmax(env)

plt.plot(softmax_rewards, label='Softmax')
plt.xlabel('Steps')
plt.ylabel('Cumulative Reward')
plt.title('Softmax Performance')
plt.legend()
plt.grid(True)
plt.show()
```
![image](https://github.com/user-attachments/assets/e9aa5448-1206-4d6a-990b-bcec5ba811d9)

### (4) çµæœåˆ†æ

- **æ™‚é–“åˆ†æï¼š** æ”¶æ–‚é€Ÿåº¦å–æ±ºæ–¼ \( \tau \) å€¼ï¼Œéé«˜æœƒé˜»ç¤™å­¸ç¿’ã€‚
- **ç©ºé–“åˆ†æï¼š** éœ€é¡å¤–è¨ˆç®— softmax åˆ†å¸ƒã€‚

---

## ğŸ¯ æ¼”ç®—æ³•å››ï¼šThompson Sampling

### (1) Algorithm Formula (LaTeX)

\[
\theta_a \sim \text{Beta}(\alpha_a, \beta_a), \quad A_t = \arg\max_a \theta_a
\]

```latex
\theta_a \sim \text{Beta}(\alpha_a, \beta_a), \quad A_t = \arg\max_a \theta_a
```

### (2) ChatGPT Prompt

> "è«‹ç”¨ç°¡å–®èªè¨€èªªæ˜ç‚ºä½• Thompson Sampling èƒ½è‡ªç„¶åœ°åœ¨æ¢ç´¢èˆ‡åˆ©ç”¨é–“å–å¾—å¹³è¡¡ï¼Œä»¥åŠ beta åˆ†å¸ƒåœ¨é€™è£¡çš„æ„ç¾©ã€‚"

### (3) ç¨‹å¼ç¢¼èˆ‡åœ–è¡¨

```python
def thompson_sampling(env, steps=1000):
    k = env.k
    alpha = np.ones(k)
    beta = np.ones(k)
    rewards = []
    cumulative = 0

    for t in range(steps):
        theta = np.random.beta(alpha, beta)
        action = np.argmax(theta)
        reward = env.pull(action)
        reward_bin = 1 if reward > 0 else 0  # æŠŠå¸¸æ…‹åˆ†å¸ƒè½‰ç‚ºä¼¯åŠªåŠ›

        alpha[action] += reward_bin
        beta[action] += 1 - reward_bin
        cumulative += reward
        rewards.append(cumulative)

    return rewards

env = BanditEnv()
ts_rewards = thompson_sampling(env)

plt.plot(ts_rewards, label='Thompson Sampling')
plt.xlabel('Steps')
plt.ylabel('Cumulative Reward')
plt.title('Thompson Sampling Performance')
plt.legend()
plt.grid(True)
plt.show()
```
![image](https://github.com/user-attachments/assets/5a58a159-e615-4ce8-8bff-f626fc7e7567)

### (4) çµæœåˆ†æ

- **æ™‚é–“åˆ†æï¼š** æ”¶æ–‚é€Ÿåº¦å¿«ï¼Œç©©å®šæ€§ä½³ã€‚
- **ç©ºé–“åˆ†æï¼š** éœ€è¨˜éŒ„ \( \alpha, \beta \) åƒæ•¸ï¼Œç•¥é«˜æ–¼å…¶ä»–æ–¹æ³•ã€‚

---

## ğŸ“Š æ‰€æœ‰æ¼”ç®—æ³•æ¯”è¼ƒåœ–è¡¨

```python
env = BanditEnv()
eps = epsilon_greedy(env)
env = BanditEnv()
ucb_ = ucb(env)
env = BanditEnv()
soft = softmax(env)
env = BanditEnv()
ts = thompson_sampling(env)

plt.figure(figsize=(10,6))
plt.plot(eps, label='Epsilon-Greedy')
plt.plot(ucb_, label='UCB')
plt.plot(soft, label='Softmax')
plt.plot(ts, label='Thompson Sampling')
plt.xlabel('Steps')
plt.ylabel('Cumulative Reward')
plt.title('Algorithm Comparison: Cumulative Reward')
plt.legend()
plt.grid(True)
plt.show()
```
![image](https://github.com/user-attachments/assets/50c8a93b-eb9a-4e35-8510-f37ffe6da88f)

---

## ğŸ” çµè«–èˆ‡å·®ç•°æ¯”è¼ƒ

| æ¼”ç®—æ³• | å„ªé» | ç¼ºé» |
|--------|------|------|
| Epsilon-Greedy | ç°¡å–®å¯¦ä½œã€æœ‰æ•ˆæ¢ç´¢ | æ¢ç´¢å›ºå®šä¸è®Šã€ä¸å¤ è‡ªé©æ‡‰ |
| UCB | ç†è«–æœ‰ä¿è­‰ã€å¼·åˆ¶æ—©æœŸæ¢ç´¢ | è¨ˆç®—è¼ƒç¹ã€å®¹æ˜“é«˜ä¼°ç½•è¦‹è‡‚ |
| Softmax | æ©Ÿç‡å¼é¸æ“‡ã€æ›´å¹³æ»‘æ¢ç´¢ | å°æº«åº¦åƒæ•¸æ•æ„Ÿã€é›£ä»¥èª¿åƒ |
| Thompson Sampling | è‡ªç„¶å¹³è¡¡æ¢ç´¢èˆ‡åˆ©ç”¨ã€æ”¶æ–‚å¿« | éœ€å‡è¨­çå‹µåˆ†å¸ƒã€æ›´æ–°è¼ƒè¤‡é›œ |

### âœ… ç©ºé–“åˆ†æ
æ‰€æœ‰æ¼”ç®—æ³•ç©ºé–“éœ€æ±‚ç‚º \( O(k) \)ï¼ŒThompson Sampling å¤šç¶­è­·å…©çµ„åƒæ•¸ã€‚

### âœ… æ™‚é–“åˆ†æ
UCB èˆ‡ Softmax è¨ˆç®—æˆæœ¬è¼ƒé«˜ï¼ŒThompson Sampling éœ€å–æ¨£ beta åˆ†å¸ƒï¼ŒEpsilon-Greedy æœ€å¿«ã€‚

