# Multi-Armed-Bandit-MAB

# HW3: Explore and Exploit for Arm-Bandit Problem

## ğŸ® ç’°å¢ƒä»‹ç´¹ï¼šå¤šè‡‚æ©Ÿå™¨äººï¼ˆMulti-Armed Banditï¼‰

æœ¬ä½œæ¥­ä¸­ï¼Œæˆ‘å€‘æ¨¡æ“¬ä¸€å€‹åŒ…å« k è‡‚çš„æ‹‰éœ¸æ©Ÿï¼Œæ¯å€‹è‡‚åœ¨æ¯æ¬¡æ‹‰ä¸‹æ™‚æœƒæ ¹æ“šå›ºå®šçš„åˆ†å¸ƒçµ¦å‡ºä¸åŒçš„å ±é…¬ã€‚é€™æ˜¯ä¸€å€‹æ¢ç´¢ï¼ˆexplorationï¼‰èˆ‡åˆ©ç”¨ï¼ˆexploitationï¼‰å…¼å…·çš„ç¶“å…¸å•é¡Œï¼Œéœ€æ‰¾å‡ºåœ¨æœ‰é™è©¦é©—æ¬¡æ•¸ä¸‹æœ€å¤§åŒ–ç´¯ç©çå‹µçš„ç­–ç•¥ã€‚

ä»¥ä¸‹æ˜¯ Python çš„æ¨¡æ“¬ç’°å¢ƒç¨‹å¼ç¢¼ï¼š

```python
import numpy as np

class BanditEnv:
    def __init__(self, k=10):
        self.k = k
        self.true_means = np.random.normal(0, 1, k)
    
    def pull(self, arm):
        return np.random.normal(self.true_means[arm], 1)
```

---

## ğŸ¯ æ¼”ç®—æ³•ä¸€ï¼šEpsilon-Greedy

### (1) Algorithm Formula (LaTeX)

![image](https://github.com/user-attachments/assets/1e93387a-dd24-432a-b5be-ca6fbdccd743)

```latex
\documentclass{article}
\usepackage{amsmath}
\usepackage[margin=1in]{geometry}  
\usepackage{tcolorbox}  

\begin{document}

\begin{center}
    \LARGE \textbf{$\varepsilon$-Greedy Algorithm}
\end{center}

\vspace{0.5em}
\hrule
\vspace{1em}

\begin{tcolorbox}[colback=gray!5, colframe=black!40, title=Action Selection Rule]
At each time step $t$, the action $A_t$ is selected as:

\[
A_t =
\begin{cases}
\arg\max\limits_a Q_t(a) & \text{with probability } 1 - \varepsilon \\
\text{random arm} & \text{with probability } \varepsilon
\end{cases}
\]

Where:
\begin{itemize}
    \item $Q_t(a)$ is the estimated value of the action $a$ at time $t$.
    \item $\varepsilon$ is a small value (e.g. 0.1) controlling exploration.
\end{itemize}
\end{tcolorbox}

\vspace{1em}
\hrule
\vspace{1em}

\begin{tcolorbox}[colback=gray!5, colframe=black!40, title=Value Update Rule]
After choosing action $a$ and receiving reward $R_t$, the value estimate is updated as:

\[
Q_{t+1}(a) = Q_t(a) + \alpha \left( R_t - Q_t(a) \right)
\]

Here, $\alpha \in (0,1]$ is the learning rate that controls how quickly the estimates adapt to new rewards.
\end{tcolorbox}

\end{document}


```

### (2) ChatGPT Prompt

> "è«‹ç°¡è¦è§£é‡‹ epsilon-greedy æ¼”ç®—æ³•å¦‚ä½•åœ¨æ¢ç´¢èˆ‡åˆ©ç”¨ä¹‹é–“å–å¾—å¹³è¡¡ï¼Œä¸¦èˆ‰ä¾‹èªªæ˜ epsilon å€¼è®ŠåŒ–å°è¡Œç‚ºçš„å½±éŸ¿ã€‚"

### (3) ç¨‹å¼ç¢¼èˆ‡åœ–è¡¨

```python
import matplotlib.pyplot as plt

def epsilon_greedy(env, epsilon=0.1, steps=1000):
    k = env.k
    Q = np.zeros(k)
    N = np.zeros(k)
    rewards = []
    cumulative = 0

    for t in range(steps):
        if np.random.rand() < epsilon:
            action = np.random.choice(k)
        else:
            action = np.argmax(Q)
        
        reward = env.pull(action)
        N[action] += 1
        Q[action] += (reward - Q[action]) / N[action]
        cumulative += reward
        rewards.append(cumulative)
    
    return rewards

env = BanditEnv()
eps_rewards = epsilon_greedy(env)

plt.plot(eps_rewards, label='Epsilon-Greedy')
plt.xlabel('Steps')
plt.ylabel('Cumulative Reward')
plt.title('Epsilon-Greedy Performance')
plt.legend()
plt.grid(True)
plt.show()
```
![image](https://github.com/user-attachments/assets/e5210a9e-e987-45ef-be65-263dc2005d0a)

### (4) çµæœåˆ†æ

- **æ™‚é–“åˆ†æï¼š** æ”¶æ–‚é€Ÿåº¦ä¾è³´æ–¼ epsilon å€¼ï¼›è¼ƒé«˜ epsilon å°è‡´é•·æœŸå­¸ç¿’æ›´æ…¢ã€‚
- **ç©ºé–“åˆ†æï¼š** åƒ…éœ€è¨˜éŒ„æ¯å€‹ arm çš„ Q å€¼èˆ‡é¸æ“‡æ¬¡æ•¸ï¼Œç©ºé–“è¤‡é›œåº¦ç‚º \( O(k) \)ã€‚

---

## ğŸ“Œ æ¼”ç®—æ³•äºŒï¼šUCB (Upper Confidence Bound)

### (1) Algorithm Formula (LaTeX)

![image](https://github.com/user-attachments/assets/0c6497c0-270d-443e-a6e2-7d7cdf7e65f7)


```latex
\documentclass{article}
\usepackage{amsmath}
\usepackage[margin=1in]{geometry}  % æ›´å¥½çœ‹ä¸€ç‚¹çš„é¡µé¢è¾¹è·
\usepackage{tcolorbox}  % ç”¨äºæ¼‚äº®çš„å†…å®¹æ¡†

\begin{document}

\begin{center}
    \LARGE \textbf{UCB Algorithm for Multi-Armed Bandit}
\end{center}

\vspace{0.5em}
\hrule
\vspace{1em}

\begin{tcolorbox}[colback=gray!5, colframe=black!40, title=Action Selection Rule]
At each time step $t$, the action $A_t$ is selected as:

\[
A_t = \arg\max_a \left[ Q_t(a) + c \cdot \sqrt{\frac{\ln t}{N_t(a)}} \right]
\]

Where:
\begin{itemize}
    \item $Q_t(a)$ is the estimated value of action $a$ at time $t$.
    \item $N_t(a)$ is the number of times action $a$ has been selected up to time $t$.
    \item $c > 0$ is a tunable parameter that balances exploration and exploitation.
    \item $\ln t$ encourages exploration of less-frequently selected actions.
\end{itemize}
\end{tcolorbox}

\vspace{1em}
\hrule
\vspace{1em}

\begin{tcolorbox}[colback=gray!5, colframe=black!40, title=Value Update Rule]
After choosing action $a$ and receiving reward $R_t$, the value estimate is updated as:

\[
Q_{t+1}(a) = Q_t(a) + \alpha \left( R_t - Q_t(a) \right)
\]

Here, $\alpha \in (0,1]$ is the learning rate that controls how quickly the estimates adapt to new rewards. Alternatively, one can use a sample average (i.e., $\alpha = \frac{1}{N_t(a)}$) for non-stationary problems.
\end{tcolorbox}

\end{document}

```

### (2) ChatGPT Prompt

> "è«‹èªªæ˜ UCB æ¼”ç®—æ³•å¦‚ä½•é€éç½®ä¿¡å€é–“é”åˆ°æ¢ç´¢èˆ‡åˆ©ç”¨çš„å¹³è¡¡ï¼Œä¸¦æŒ‡å‡ºå¸¸æ•¸ c å°è¡Œç‚ºçš„å½±éŸ¿ã€‚"

### (3) ç¨‹å¼ç¢¼èˆ‡åœ–è¡¨

```python
def ucb(env, c=2, steps=1000):
    k = env.k
    Q = np.zeros(k)
    N = np.zeros(k)
    rewards = []
    cumulative = 0

    for t in range(1, steps + 1):
        ucb_values = np.where(N > 0, Q + c * np.sqrt(np.log(t) / N), float('inf'))
        action = np.argmax(ucb_values)
        reward = env.pull(action)

        N[action] += 1
        Q[action] += (reward - Q[action]) / N[action]
        cumulative += reward
        rewards.append(cumulative)
    
    return rewards

env = BanditEnv()
ucb_rewards = ucb(env)

plt.plot(ucb_rewards, label='UCB')
plt.xlabel('Steps')
plt.ylabel('Cumulative Reward')
plt.title('UCB Performance')
plt.legend()
plt.grid(True)
plt.show()

```
![image](https://github.com/user-attachments/assets/0085c65d-3f4a-4b98-8021-2fd81cd55f0f)

### (4) çµæœåˆ†æ

- **æ™‚é–“åˆ†æï¼š** åˆæœŸæ¢ç´¢æ•ˆæœä½³ï¼Œæ”¶æ–‚é€Ÿåº¦å¿«ã€‚
- **ç©ºé–“åˆ†æï¼š** èˆ‡ epsilon-greedy ç›¸åŒï¼Œè¨ˆç®—æˆæœ¬å¢åŠ ã€‚

---

## ğŸ² æ¼”ç®—æ³•ä¸‰ï¼šSoftmax

### (1) Algorithm Formula (LaTeX)

![image](https://github.com/user-attachments/assets/665bcca3-6b49-4c48-a4b5-604a8857b183)


```latex
\documentclass{article}
\usepackage{amsmath}
\usepackage[margin=1in]{geometry}  % æ›´å¥½çœ‹ä¸€ç‚¹çš„é¡µé¢è¾¹è·
\usepackage{tcolorbox}  % ç”¨äºæ¼‚äº®çš„å†…å®¹æ¡†

\begin{document}

\begin{center}
    \LARGE \textbf{Softmax Algorithm for Multi-Armed Bandit}
\end{center}

\vspace{0.5em}
\hrule
\vspace{1em}

\begin{tcolorbox}[colback=gray!5, colframe=black!40, title=Action Selection Rule]
At each time step $t$, the probability of selecting action $a$ is computed using the softmax function:

\[
P_t(a) = \frac{e^{Q_t(a)/\tau}}{\sum\limits_b e^{Q_t(b)/\tau}}
\]

Then, action $A_t$ is sampled according to the distribution $P_t(a)$.

Where:
\begin{itemize}
    \item $Q_t(a)$ is the estimated value of action $a$ at time $t$.
    \item $\tau > 0$ is the \textbf{temperature} parameter that controls the randomness of action selection:
    \begin{itemize}
        \item High $\tau$: more exploration (actions have similar probabilities)
        \item Low $\tau$: more exploitation (focuses on high-value actions)
    \end{itemize}
\end{itemize}
\end{tcolorbox}

\vspace{1em}
\hrule
\vspace{1em}

\begin{tcolorbox}[colback=gray!5, colframe=black!40, title=Value Update Rule]
After choosing action $a$ and receiving reward $R_t$, the value estimate is updated as:

\[
Q_{t+1}(a) = Q_t(a) + \alpha \left( R_t - Q_t(a) \right)
\]

Here, $\alpha \in (0,1]$ is the learning rate that determines how quickly the estimate adapts to new information. A decaying or sample-average approach to $\alpha$ is also commonly used.
\end{tcolorbox}

\end{document}

```

### (2) ChatGPT Prompt

> "è«‹ç”¨ç°¡å–®ä¾‹å­èªªæ˜ Softmax ç­–ç•¥å¦‚ä½•æ ¹æ“šä¸åŒæº«åº¦åƒæ•¸ï¼ˆ\tauï¼‰èª¿æ•´æ¢ç´¢ç¨‹åº¦ï¼Œä¸¦è§£é‡‹ç‚ºä½• \tau è¶Šå¤§è¶Šå‚¾å‘æ¢ç´¢ã€‚"

### (3) ç¨‹å¼ç¢¼èˆ‡åœ–è¡¨

```python
def softmax(env, tau=0.1, steps=1000):
    k = env.k
    Q = np.zeros(k)
    N = np.zeros(k)
    rewards = []
    cumulative = 0

    for t in range(steps):
        exp_Q = np.exp(Q / tau)
        probs = exp_Q / np.sum(exp_Q)
        action = np.random.choice(k, p=probs)
        
        reward = env.pull(action)
        N[action] += 1
        Q[action] += (reward - Q[action]) / N[action]
        cumulative += reward
        rewards.append(cumulative)
    
    return rewards

env = BanditEnv()
softmax_rewards = softmax(env)

plt.plot(softmax_rewards, label='Softmax')
plt.xlabel('Steps')
plt.ylabel('Cumulative Reward')
plt.title('Softmax Performance')
plt.legend()
plt.grid(True)
plt.show()
```
![image](https://github.com/user-attachments/assets/e9aa5448-1206-4d6a-990b-bcec5ba811d9)

### (4) çµæœåˆ†æ

- **æ™‚é–“åˆ†æï¼š** æ”¶æ–‚é€Ÿåº¦å–æ±ºæ–¼ \( \tau \) å€¼ï¼Œéé«˜æœƒé˜»ç¤™å­¸ç¿’ã€‚
- **ç©ºé–“åˆ†æï¼š** éœ€é¡å¤–è¨ˆç®— softmax åˆ†å¸ƒã€‚

---

## ğŸ¯ æ¼”ç®—æ³•å››ï¼šThompson Sampling

### (1) Algorithm Formula (LaTeX)

![image](https://github.com/user-attachments/assets/9ca620c3-da84-4243-b4f6-b33f1742bfa8)


```latex
\documentclass{article}
\usepackage{amsmath}
\usepackage[margin=1in]{geometry}  % æ›´å¥½çœ‹ä¸€ç‚¹çš„é¡µé¢è¾¹è·
\usepackage{tcolorbox}  % ç”¨äºæ¼‚äº®çš„å†…å®¹æ¡†

\begin{document}

\begin{center}
    \LARGE \textbf{Thompson Sampling for Multi-Armed Bandit}
\end{center}

\vspace{0.5em}
\hrule
\vspace{1em}

\begin{tcolorbox}[colback=gray!5, colframe=black!40, title=Action Selection Rule]
At each time step $t$, the agent maintains a posterior distribution over the reward of each action. The action $A_t$ is selected as:

\[
A_t = \arg\max_a \theta_a
\]

Where:
\begin{itemize}
    \item For each action $a$, sample $\theta_a$ from its posterior distribution $P(\theta_a \mid \text{data})$.
    \item Choose the action $a$ with the highest sampled $\theta_a$.
\end{itemize}

\textbf{Typical case: Bernoulli rewards}

\begin{itemize}
    \item Use Beta distribution as the conjugate prior.
    \item Maintain parameters $(\alpha_a, \beta_a)$ for each action $a$:
    \[
    \theta_a \sim \text{Beta}(\alpha_a, \beta_a)
    \]
\end{itemize}
\end{tcolorbox}

\vspace{1em}
\hrule
\vspace{1em}

\begin{tcolorbox}[colback=gray!5, colframe=black!40, title=Posterior Update Rule]
After choosing action $a$ and observing reward $R_t \in \{0,1\}$ (e.g., Bernoulli reward), update the Beta distribution parameters:

\[
\alpha_a \leftarrow \alpha_a + R_t, \quad \beta_a \leftarrow \beta_a + (1 - R_t)
\]

This keeps the posterior up to date:
\[
\theta_a \sim \text{Beta}(\alpha_a, \beta_a)
\]
\end{tcolorbox}

\end{document}

```

### (2) ChatGPT Prompt

> "è«‹ç”¨ç°¡å–®èªè¨€èªªæ˜ç‚ºä½• Thompson Sampling èƒ½è‡ªç„¶åœ°åœ¨æ¢ç´¢èˆ‡åˆ©ç”¨é–“å–å¾—å¹³è¡¡ï¼Œä»¥åŠ beta åˆ†å¸ƒåœ¨é€™è£¡çš„æ„ç¾©ã€‚"

### (3) ç¨‹å¼ç¢¼èˆ‡åœ–è¡¨

```python
def thompson_sampling(env, steps=1000):
    k = env.k
    alpha = np.ones(k)
    beta = np.ones(k)
    rewards = []
    cumulative = 0

    for t in range(steps):
        theta = np.random.beta(alpha, beta)
        action = np.argmax(theta)
        reward = env.pull(action)
        reward_bin = 1 if reward > 0 else 0  # æŠŠå¸¸æ…‹åˆ†å¸ƒè½‰ç‚ºä¼¯åŠªåŠ›

        alpha[action] += reward_bin
        beta[action] += 1 - reward_bin
        cumulative += reward
        rewards.append(cumulative)

    return rewards

env = BanditEnv()
ts_rewards = thompson_sampling(env)

plt.plot(ts_rewards, label='Thompson Sampling')
plt.xlabel('Steps')
plt.ylabel('Cumulative Reward')
plt.title('Thompson Sampling Performance')
plt.legend()
plt.grid(True)
plt.show()
```
![image](https://github.com/user-attachments/assets/5a58a159-e615-4ce8-8bff-f626fc7e7567)

### (4) çµæœåˆ†æ

- **æ™‚é–“åˆ†æï¼š** æ”¶æ–‚é€Ÿåº¦å¿«ï¼Œç©©å®šæ€§ä½³ã€‚
- **ç©ºé–“åˆ†æï¼š** éœ€è¨˜éŒ„ \( \alpha, \beta \) åƒæ•¸ï¼Œç•¥é«˜æ–¼å…¶ä»–æ–¹æ³•ã€‚

---

## ğŸ“Š æ‰€æœ‰æ¼”ç®—æ³•æ¯”è¼ƒåœ–è¡¨

```python
env = BanditEnv()
eps = epsilon_greedy(env)
env = BanditEnv()
ucb_ = ucb(env)
env = BanditEnv()
soft = softmax(env)
env = BanditEnv()
ts = thompson_sampling(env)

plt.figure(figsize=(10,6))
plt.plot(eps, label='Epsilon-Greedy')
plt.plot(ucb_, label='UCB')
plt.plot(soft, label='Softmax')
plt.plot(ts, label='Thompson Sampling')
plt.xlabel('Steps')
plt.ylabel('Cumulative Reward')
plt.title('Algorithm Comparison: Cumulative Reward')
plt.legend()
plt.grid(True)
plt.show()
```
![image](https://github.com/user-attachments/assets/50c8a93b-eb9a-4e35-8510-f37ffe6da88f)

---

## ğŸ” çµè«–èˆ‡å·®ç•°æ¯”è¼ƒ

| æ¼”ç®—æ³• | å„ªé» | ç¼ºé» |
|--------|------|------|
| Epsilon-Greedy | ç°¡å–®å¯¦ä½œã€æœ‰æ•ˆæ¢ç´¢ | æ¢ç´¢å›ºå®šä¸è®Šã€ä¸å¤ è‡ªé©æ‡‰ |
| UCB | ç†è«–æœ‰ä¿è­‰ã€å¼·åˆ¶æ—©æœŸæ¢ç´¢ | è¨ˆç®—è¼ƒç¹ã€å®¹æ˜“é«˜ä¼°ç½•è¦‹è‡‚ |
| Softmax | æ©Ÿç‡å¼é¸æ“‡ã€æ›´å¹³æ»‘æ¢ç´¢ | å°æº«åº¦åƒæ•¸æ•æ„Ÿã€é›£ä»¥èª¿åƒ |
| Thompson Sampling | è‡ªç„¶å¹³è¡¡æ¢ç´¢èˆ‡åˆ©ç”¨ã€æ”¶æ–‚å¿« | éœ€å‡è¨­çå‹µåˆ†å¸ƒã€æ›´æ–°è¼ƒè¤‡é›œ |

### âœ… ç©ºé–“åˆ†æ
æ‰€æœ‰æ¼”ç®—æ³•ç©ºé–“éœ€æ±‚ç‚º \( O(k) \)ï¼ŒThompson Sampling å¤šç¶­è­·å…©çµ„åƒæ•¸ã€‚

### âœ… æ™‚é–“åˆ†æ
UCB èˆ‡ Softmax è¨ˆç®—æˆæœ¬è¼ƒé«˜ï¼ŒThompson Sampling éœ€å–æ¨£ beta åˆ†å¸ƒï¼ŒEpsilon-Greedy æœ€å¿«ã€‚

